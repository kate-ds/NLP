{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача поиска похожих по эмбеддингам\n",
    "Скачиваем датасет (источник): положительные, отрицательные.\n",
    "\n",
    "1. объединить в одну выборку\n",
    "2. на основе word2vec/fasttext/glove/слоя Embedding реализовать метод поиска ближайших твитов\n",
    "(на вход метода должен приходить запрос (какой-то твит, вопрос) и количество вариантов вывода к примеру 5-ть, ваш метод должен возвращать 5-ть ближайших твитов к этому запросу)\n",
    "3. Проверить насколько хорошо работают подходы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-06-07 17:26:35--  https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.70.18, 2620:100:6026:18::a27d:4612\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.70.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/raw/fnpq3z4bcnoktiv/positive.csv [following]\n",
      "--2022-06-07 17:26:35--  https://www.dropbox.com/s/raw/fnpq3z4bcnoktiv/positive.csv\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc89f103c27c13694751c268ee27.dl.dropboxusercontent.com/cd/0/inline/BmwWNWzR1gQcziG43Q3CH9mzRRD5a_yGfK6rlbYuYrRzX-QJ6vsK0pTlCIg-iatBX638qK4Ot9zCzfVwonRHgDKphmD-ZFVzldYLMnS-ktoOyBK5kjHMBu2t2wDTZg-dkWSaZfwna2N0tNTHlJ6O05LIpQSYobMs-X-LqdLKKIgIkA/file# [following]\n",
      "--2022-06-07 17:26:36--  https://uc89f103c27c13694751c268ee27.dl.dropboxusercontent.com/cd/0/inline/BmwWNWzR1gQcziG43Q3CH9mzRRD5a_yGfK6rlbYuYrRzX-QJ6vsK0pTlCIg-iatBX638qK4Ot9zCzfVwonRHgDKphmD-ZFVzldYLMnS-ktoOyBK5kjHMBu2t2wDTZg-dkWSaZfwna2N0tNTHlJ6O05LIpQSYobMs-X-LqdLKKIgIkA/file\n",
      "Resolving uc89f103c27c13694751c268ee27.dl.dropboxusercontent.com (uc89f103c27c13694751c268ee27.dl.dropboxusercontent.com)... 162.125.70.15, 2620:100:6026:15::a27d:460f\n",
      "Connecting to uc89f103c27c13694751c268ee27.dl.dropboxusercontent.com (uc89f103c27c13694751c268ee27.dl.dropboxusercontent.com)|162.125.70.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 26233379 (25M) [text/plain]\n",
      "Saving to: ‘positive.csv’\n",
      "\n",
      "positive.csv        100%[===================>]  25.02M  15.9MB/s    in 1.6s    \n",
      "\n",
      "2022-06-07 17:26:38 (15.9 MB/s) - ‘positive.csv’ saved [26233379/26233379]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-06-07 17:39:57--  https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.70.18, 2620:100:6026:18::a27d:4612\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.70.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/raw/r6u59ljhhjdg6j0/negative.csv [following]\n",
      "--2022-06-07 17:39:58--  https://www.dropbox.com/s/raw/r6u59ljhhjdg6j0/negative.csv\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc2041f82317e88971495f6d18e4.dl.dropboxusercontent.com/cd/0/inline/Bmx-9b1KSDT85hpKwQ4m3eKkSPnqp9G0xC75YU8xu3hykumC5nyM_BQS1ZTiPBZ1ysWB1ls-9TUslx7DNtQBIrUtKA1ANdjYT4e2h4jPhr0LFRXs3Ygcw5P-eRnatRL8M1RRsTTmkXQjRWMLdIL27RbExPbiDOc8xL06X7p1A8D9MA/file# [following]\n",
      "--2022-06-07 17:39:58--  https://uc2041f82317e88971495f6d18e4.dl.dropboxusercontent.com/cd/0/inline/Bmx-9b1KSDT85hpKwQ4m3eKkSPnqp9G0xC75YU8xu3hykumC5nyM_BQS1ZTiPBZ1ysWB1ls-9TUslx7DNtQBIrUtKA1ANdjYT4e2h4jPhr0LFRXs3Ygcw5P-eRnatRL8M1RRsTTmkXQjRWMLdIL27RbExPbiDOc8xL06X7p1A8D9MA/file\n",
      "Resolving uc2041f82317e88971495f6d18e4.dl.dropboxusercontent.com (uc2041f82317e88971495f6d18e4.dl.dropboxusercontent.com)... 162.125.70.15, 2620:100:6026:15::a27d:460f\n",
      "Connecting to uc2041f82317e88971495f6d18e4.dl.dropboxusercontent.com (uc2041f82317e88971495f6d18e4.dl.dropboxusercontent.com)|162.125.70.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 24450101 (23M) [text/plain]\n",
      "Saving to: ‘negative.csv’\n",
      "\n",
      "negative.csv        100%[===================>]  23.32M  19.6MB/s    in 1.2s    \n",
      "\n",
      "2022-06-07 17:40:00 (19.6 MB/s) - ‘negative.csv’ saved [24450101/24450101]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticon_dict = {\n",
    "\":)\": \"счастливый\",\n",
    "\":‑)\": \"счастливый\",\n",
    "\":-]\": \"счастливый\",\n",
    "\":-3\": \"счастливый\",\n",
    "\":->\": \"счастливый\",\n",
    "\"8-)\": \"счастливый\",\n",
    "\":-}\": \"счастливый\",\n",
    "\":o)\": \"счастливый\",\n",
    "\":c)\": \"счастливый\",\n",
    "\":^)\": \"счастливый\",\n",
    "\"=]\": \"счастливый\",\n",
    "\"=)\": \"счастливый\",\n",
    "\"<3\": \"счастливый\",\n",
    "\":-(\": \"грустный\",\n",
    "\":(\": \"грустный\",\n",
    "\":c\": \"грустный\",\n",
    "\":<\": \"грустный\",\n",
    "\":[\": \"грустный\",\n",
    "\">:[\": \"грустный\",\n",
    "\":{\": \"грустный\",\n",
    "\">:(\": \"грустный\",\n",
    "\":-c\": \"грустный\",\n",
    "\":-< \": \"грустный\",\n",
    "\":-[\": \"грустный\",\n",
    "\":-||\": \"грустный\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/sokolovaeka@ozon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/sokolovaeka@ozon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import annoy\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import gensim.downloader as ap\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from stop_words import get_stop_words\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from stop_words import get_stop_words\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting annoy\n",
      "  Downloading annoy-1.17.0.tar.gz (646 kB)\n",
      "\u001b[K     |████████████████████████████████| 646 kB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: annoy\n",
      "  Building wheel for annoy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for annoy: filename=annoy-1.17.0-cp37-cp37m-linux_x86_64.whl size=394717 sha256=35de7b80a002add9cc8f030aca8bdf9d98d628b52202db7ce9c100739278be63\n",
      "  Stored in directory: /home/sokolovaeka@ozon/.cache/pip/wheels/4f/e8/1e/7cc9ebbfa87a3b9f8ba79408d4d31831d67eea918b679a4c07\n",
      "Successfully built annoy\n",
      "Installing collected packages: annoy\n",
      "Successfully installed annoy-1.17.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = pd.read_csv('positive.csv', delimiter=';', usecols=[3,4], names=['tweet', 'intent'])\n",
    "negative = pd.read_csv('negative.csv', delimiter=';', usecols=[3,4], names=['tweet',  'intent'])\n",
    "\n",
    "df = pd.concat([positive, negative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    111923\n",
       "Name: intent, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative.intent.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sokolovaeka@ozon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def user_del2(tweet):\n",
    "    \"\"\"Delete @user from tweet\"\"\"       \n",
    "    return re.sub(r'@[\\w]*', '', tweet)\n",
    "\n",
    "\n",
    "def change_from_dict(tweet, dictionary): \n",
    "    \"\"\"Chenge element for value from dict if exists\"\"\"\n",
    "    full_text = [dictionary.get(word) or word for word in tweet.split(' ')]\n",
    "    return ' '.join(full_text)\n",
    "\n",
    "def tweets_preprocessing(example): \n",
    "\n",
    "    # Удалим @user из всех твитов\n",
    "    example = user_del2(example)\n",
    "    # Изменим регистр твитов на нижний \n",
    "    example = example.lower()\n",
    "    # Заменим сокращения с апострофами \n",
    "#     example = change_from_dict(example, short_word_dict)\n",
    "    # Заменим эмотиконы\n",
    "    example = change_from_dict(example, emoticon_dict)\n",
    "    # Заменим пунктуацию на пробелы\n",
    "    example = re.sub(r'[^\\w\\s]', ' ', example)\n",
    "    # Заменим спец. символы на пробелы\n",
    "    example = re.sub(r'[^a-zA-Zа-яёA-ЯЁ0-9]', ' ', example)\n",
    "    # Заменим числа на пробелы\n",
    "    example = re.sub(r'[^a-zA-Zа-яёA-ЯЁ]', ' ', example)\n",
    "    # Удалим из текста слова длиной в 2 символа\n",
    "    example = [w for w in example.split() if len(w)>2]\n",
    "    # Удалим стоп-слова\n",
    "    stop_words = get_stop_words(\"ru\") + stopwords.words('russian')\n",
    "    example = [w for w in example if w not in stop_words] \n",
    "    # Приведем к нормальной форме\n",
    "    morph = MorphAnalyzer()\n",
    "    example = [morph.parse(w)[0].normal_form for w in example]  \n",
    "    \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>intent</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@first_timee хоть я и школота, но поверь, у на...</td>\n",
       "      <td>1</td>\n",
       "      <td>[школотый, поверь, самый, общество, профилиров...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Да, все-таки он немного похож на него. Но мой ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[таки, похожий, мальчик, равно]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @KatiaCheh: Ну ты идиотка) я испугалась за ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[идиотка, испугаться]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @digger2912: \"Кто то в углу сидит и погибае...</td>\n",
       "      <td>1</td>\n",
       "      <td>[угол, сидеть, погибать, голод, порция, взять,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@irina_dyshkant Вот что значит страшилка :D\\nН...</td>\n",
       "      <td>1</td>\n",
       "      <td>[страшилка, блин, посмотреть, часть, создаться...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  intent  \\\n",
       "0  @first_timee хоть я и школота, но поверь, у на...       1   \n",
       "1  Да, все-таки он немного похож на него. Но мой ...       1   \n",
       "2  RT @KatiaCheh: Ну ты идиотка) я испугалась за ...       1   \n",
       "3  RT @digger2912: \"Кто то в углу сидит и погибае...       1   \n",
       "4  @irina_dyshkant Вот что значит страшилка :D\\nН...       1   \n",
       "\n",
       "                                        preprocessed  \n",
       "0  [школотый, поверь, самый, общество, профилиров...  \n",
       "1                    [таки, похожий, мальчик, равно]  \n",
       "2                              [идиотка, испугаться]  \n",
       "3  [угол, сидеть, погибать, голод, порция, взять,...  \n",
       "4  [страшилка, блин, посмотреть, часть, создаться...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['preprocessed'] = df.tweet.apply(tweets_preprocessing)\n",
    "df.reset_index(drop=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>intent</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@first_timee хоть я и школота, но поверь, у на...</td>\n",
       "      <td>1</td>\n",
       "      <td>['школотый', 'поверь', 'самый', 'общество', 'п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Да, все-таки он немного похож на него. Но мой ...</td>\n",
       "      <td>1</td>\n",
       "      <td>['таки', 'похожий', 'мальчик', 'равно']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  intent  \\\n",
       "0  @first_timee хоть я и школота, но поверь, у на...       1   \n",
       "1  Да, все-таки он немного похож на него. Но мой ...       1   \n",
       "\n",
       "                                        preprocessed  \n",
       "0  ['школотый', 'поверь', 'самый', 'общество', 'п...  \n",
       "1            ['таки', 'похожий', 'мальчик', 'равно']  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.to_csv('prepared_tweets.csv')\n",
    "# df = pd.read_csv('prepared_tweets.csv')\n",
    "df = df[['tweet', 'intent', 'preprocessed']]\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['школотый', 'поверь', 'самый', 'общество', 'профилировать', 'предмет', 'тип']\",\n",
       " \"['таки', 'похожий', 'мальчик', 'равно']\",\n",
       " \"['идиотка', 'испугаться']\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = [tweet for tweet in df['preprocessed'].values if len(tweet)>2]\n",
    "tweets[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelW2V = Word2Vec(sentences=tweets, vector_size=300, window=5, min_count=1)\n",
    "modelFT = FastText(sentences=tweets, vector_size=300, min_count=1, window=5, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_index = annoy.AnnoyIndex(300 ,'angular')\n",
    "ft_index = annoy.AnnoyIndex(300 ,'angular')\n",
    "\n",
    "index_map = {}\n",
    "counter = 0\n",
    "\n",
    "for line in df['preprocessed']:\n",
    "    n_w2v = 0\n",
    "    n_ft = 0\n",
    "#     spls = line.split(\"\\t\")\n",
    "    index_map[counter] = line\n",
    "\n",
    "    vector_w2v = np.zeros(300)\n",
    "    vector_ft = np.zeros(300)\n",
    "    for word in line:\n",
    "        if word in modelW2V.wv:\n",
    "            vector_w2v += modelW2V.wv[word]\n",
    "            n_w2v += 1\n",
    "        if word in modelFT.wv:\n",
    "            vector_ft += modelFT.wv[word]\n",
    "            n_ft += 1\n",
    "    if n_w2v > 0:\n",
    "        vector_w2v = vector_w2v / n_w2v\n",
    "    if n_ft > 0:\n",
    "        vector_ft = vector_ft / n_ft\n",
    "    w2v_index.add_item(counter, vector_w2v)\n",
    "    ft_index.add_item(counter, vector_ft)\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "    if counter > 100000:\n",
    "        break\n",
    "\n",
    "w2v_index.build(10)\n",
    "ft_index.build(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(tweet, index, model, index_map):\n",
    "    tweet = tweets_preprocessing(tweet)\n",
    "    vector = np.zeros(300)\n",
    "    norm = 0\n",
    "    for word in tweet:\n",
    "        if word in model.wv:\n",
    "            vector += model.wv[word]\n",
    "            norm += 1\n",
    "    if norm > 0:\n",
    "        vector = vector / norm\n",
    "    answers = index.get_nns_by_vector(vector, 5, )\n",
    "    return [df.tweet.values[i] for i in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweet = 'Сегодня хорошая погода'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Только сейчас с трени пришла......на улице наступила настоящая зима.Красотень то какая:*',\n",
       " 'Зашла в аудиторию,как получила под кофту снежка!от Оли Пупач) спасибо)',\n",
       " '@Radkat100 @OlyaValsbaston измени установку: должно быть отлично)))',\n",
       " '@Kisa_Kris если через контору снимать не меньше 12 это точно , 2 недели назад узнавали ) а если сами , то там уже на совесть цены',\n",
       " 'скиди, ты не тупая, а это я СТО ПРОЦ, если еще будет неправильно то, что она у меня списала........ то мне не жить, хотя это даже КУЛЛ)']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response(test_tweet, w2v_index, modelW2V, index_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@Valery_Valeriya пахахахахахахахахахахахахахахахахахахах :DDDDDDDD',\n",
       " '@kaa_musakaeva хахаахаххахахаххххахахаахахахахх))))))',\n",
       " '@v_strepetov @vlad__nvp  АХАХАХАХАХХАХАХАХАХАХАХАХАХАХАХА !):DD',\n",
       " '@HappyNataly_Sib ахахахахахахахахахахахах лишь бы не меньше)))',\n",
       " 'RT @MAPCOKEMOB_MHE: @yulechka_minion аххахаххахахахах \\n:D']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response(test_tweet, ft_index, modelFT, index_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель word2vec показала более-менее похожие по смыслу твиты, FastText что-то плохо отработала"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_env",
   "language": "python",
   "name": "python3_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
